{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb2f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204df5eb",
   "metadata": {},
   "source": [
    "#### Read the Data from the CSV Source File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a4cf942",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('data/targetfirm_prediction_dataset_small.csv')\n",
    "raw_data = raw_data.fillna(0)\n",
    "data = np.array(raw_data.values)\n",
    "data = data[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c108c",
   "metadata": {},
   "source": [
    "#### Get the indices of the rows with the target for each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32f36ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GVKEY Labels\n",
    "labels = data[:,0]\n",
    "_,ind1,inv1,cou1 = np.unique(labels, return_index=True, return_inverse=True, return_counts=True)\n",
    "# Index of the last occurrence of the GVKEY row in the dataset\n",
    "# print((ind1+cou1-1))\n",
    "\n",
    "target_indices = ind1+cou1-1\n",
    "# print(\"Length of labels is \", len(target_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b1f67d",
   "metadata": {},
   "source": [
    "Since we can use at most the previous 5 years data to predict the next year, \n",
    "we split and prepare the data accordingly. \n",
    "\n",
    "As per the example: For the year 2001, there is only one historical data point that can be used.\n",
    "\n",
    "But for the year 2010, there are five data points that can be used for prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c1496b",
   "metadata": {},
   "source": [
    "We use a window size of 5 (for the maximum number of years that can be traced back). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ced95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_and_split(source_data, window_size, target_indices):\n",
    "    returndata = [ ]\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for i in target_indices:\n",
    "        current_index = i\n",
    "        previous_index = current_index - 1\n",
    "        count = 0 \n",
    "        while(source_data[:,1][current_index] > source_data[:,1][previous_index] and count < window_size):\n",
    "            current_index-=1\n",
    "            previous_index = current_index - 1\n",
    "            count+=1\n",
    "        if(current_index == i):\n",
    "#             print(\"Skipping this\")\n",
    "            continue\n",
    "\n",
    "        x_data.append(source_data[current_index:i,3:17])\n",
    "        y_data.append(source_data[i,2])\n",
    "        returndata.append((source_data[current_index:i,3:17], source_data[i,2]))\n",
    "\n",
    "    test_size = int(np.round(0.3 * len(returndata)))\n",
    "    \n",
    "    train = returndata[:-test_size]\n",
    "    x_train = x_data[:-test_size]\n",
    "    y_train = y_data[:-test_size]\n",
    "    \n",
    "    test = returndata[-test_size:]\n",
    "    x_test = x_data[-test_size:]\n",
    "    y_test = y_data[-test_size:]\n",
    "    \n",
    "    return train, test, x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b25490",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_window = 5 # Years\n",
    "data_tensor = torch.FloatTensor(data)\n",
    "\n",
    "train, test, x_train, y_train, x_test, y_test = prepare_data_and_split(data_tensor,year_window,target_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d5360",
   "metadata": {},
   "source": [
    "We use padding (pytorch - pad_sequence) to pad the sequence for the years when there is less than 5 year data available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c4858",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequence(x_train, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c7521",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 14\n",
    "hidden_size = 100 \n",
    "num_layers = 2 \n",
    "output_size = 1 \n",
    "num_epochs = 100 \n",
    "\n",
    "# Change here for the below models\n",
    "learning_rate = 0.05 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb42d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size \n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "#         self.h_cell = (torch.zeros(self.num_layers,1, self.hidden_size),\n",
    "#                        torch.zeros(self.num_layers,1, self.hidden_size))\n",
    "\n",
    "        \n",
    "    def forward(self,x): \n",
    "#         out, self.h_cell = self.lstm(x.view(len(x),1,-1),self.h_cell)\n",
    "        h0 = torch.zeros(self.num_layers, 1, self.hidden_size).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, 1, self.hidden_size).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        output = self.fc(out.view(len(x),-1))\n",
    "        return output[-1]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d42f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM(input_size = input_size, \n",
    "                  hidden_size = hidden_size, \n",
    "                  num_layers = num_layers, \n",
    "                  output_size = output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist = np.zeros(num_epochs)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), learning_rate)\n",
    "for i in range(num_epochs):\n",
    "    for training_data, y_lstm_targets in train:\n",
    "\n",
    "        y_train_pred = lstm_model(training_data)\n",
    "        loss = criterion(y_train_pred, y_lstm_targets)\n",
    "        print(\"Epoch \", i, \"MSE: \", loss.item())\n",
    "        hist[i] = loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "#         loss.backward(retain_graph=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(\"Time taken for the training is \", training_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93312a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn) = self.gru(x, (h0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf76b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ed03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "gru = []\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    y_train_pred = model(x_train)\n",
    "\n",
    "    loss = criterion(y_train_pred, y_train_gru)\n",
    "    print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "    hist[t] = loss.item()\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "training_time = time.time()-start_time    \n",
    "print(\"Training time: {}\".format(training_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
